{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Cell Attention Network (CAN)\n",
    "\n",
    "We create and train a  Cell Attention Network (CAN) originally proposed in [Giusti et al. Cell Attention Networks (2022)](https://arxiv.org/abs/2209.08179). The aim of this notebook is to be didactic and clear, for further technical and implementation details please refer to the original paper and the TopoModelX documentation.\n",
    "\n",
    "### Abstract:\n",
    "\n",
    "Since their introduction, graph attention networks achieved outstanding results in graph representation learning tasks. However, these networks consider only pairwise relationships among nodes and then they are not able to fully exploit higher-order interactions present in many real world data-sets. In this paper, we introduce Cell Attention Networks (CANs), a neural architecture operating on data defined over the vertices of a graph, representing the graph as the 1-skeleton of a cell complex introduced to capture higher order interactions. In particular, we exploit the lower and upper neighborhoods, as encoded in the cell complex, to design two independent masked self-attention mechanisms, thus generalizing the conventional graph attention strategy. The approach used in CANs is hierarchical and it incorporates the following steps: i) a lifting algorithm that learns edge features from node features; ii) a cell attention mechanism to find the optimal combination of edge features over both lower and upper neighbors; iii) a hierarchical edge pooling mechanism to extract a compact meaningful set of features. \n",
    "\n",
    "<center>\n",
    "        <a href=\"https://ibb.co/1JHND1j\"><img src=\"https://i.ibb.co/YTvSzmw/98d25e90-4216-4d4d-975c-2baa3e388f1c.jpg\" alt=\"98d25e90-4216-4d4d-975c-2baa3e388f1c\"></a>\n",
    "        <figcaption></figcaption>\n",
    "</center>\n",
    "\n",
    "**Remark.** The notation we use is defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)and [Hajij et al : Topological Deep Learning: Going Beyond Graph Data(2023)](https://arxiv.org/pdf/2206.00606.pdf). Custom symbols are introduced along the notebook, when necessary.\n",
    "\n",
    "### The Neural Network:\n",
    "\n",
    "The CAN layer, in the original paper, takes rank-$0$ signals as input  and gives rank-$0$ signals as output (in general, it could take rank-$r$ signals as input  and give rank-$r$ signals as output). The involved neighborhoods are: $N = \\{\\mathcal N_1, \\mathcal N_2\\} = \\{A_{\\uparrow,r+1}, A_{\\downarrow, r+1}\\}$.\n",
    "\n",
    "A CAN layer is made by the following 3 message passing stages:\n",
    "\n",
    "1) Attentional Lift (to compute $r+1$-signals from $r$-signals):\n",
    "\n",
    "\\begin{align*}\n",
    "&游린\\textrm{ Message.} \\quad m_{(y,z) \\rightarrow x} &=& \\alpha(h_y^0,h_z^0) = \\\\\n",
    "        &&=&\\Theta \\cdot (h_y^0||h_z^0)\\\\\n",
    "&游릱\\textrm{ Update.} \\quad h_x^1 &=& \\phi(h_x^0,  m_{(y,z) \\rightarrow x})\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha$ is a learnable function parameterized by $\\Theta$ $\\in$ $\\mathbb R^{2F_0 \\times H}$. In the case of node signals as input, $F_0$ is the number of nodes' features and $H$ is the number of heads as defined in the original paper.\n",
    "- $||$ is the concatenation operator.\n",
    "- $\\phi$ is a learnable function that updates the features of a cell.\n",
    "\n",
    "2) ($\\times L$) Attentional message passing at level $r+1$. The general equation is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{h}_x^{t+1} =  \\phi^t \\Bigg ( \\textbf{h}_x^{t}, \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}\\bigoplus_{y \\in \\mathcal{N}_k(x)}  \\alpha_k(h_x^t,h_y^t)\\Bigg ) \n",
    "\\end{align*}\n",
    "\n",
    "In detail:\n",
    "\n",
    "\\begin{align*}\n",
    "&游린\\textrm{ Message.} &\\quad m_{(y \\rightarrow x),k} =&\n",
    "\\alpha_k(h_x^t,h_y^t) =\n",
    "a_k(h_x^{t}, h_y^{t}) \\cdot \\psi_k^t(h_x^{t})\\quad \\forall \\mathcal N_k \\in \\mathcal{N}\\\\\n",
    "\\\\\n",
    "&游릲 \\textrm{ Within-Neighborhood Aggregation.} &\\quad m_{x,k}               =& \\bigoplus_{y \\in \\mathcal{N}_k(x)}  m_{(y \\rightarrow x),k}\\\\\n",
    "\\\\\n",
    "&游릴 \\textrm{ Between-Neighborhood Aggregation.} &\\quad m_{x} =& \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}m_{x,k}\\\\\n",
    "\\\\\n",
    "&游릱 \\textrm{ Update.}&\\quad h_x^{t+1}                =& \\phi^{t}(h_x^t, m_{x})\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\psi_k^t$ is a learnable function that computes the importance of a $r+1$-cell.\n",
    "- $a_k^t: \\mathbb R^{F^l}\\times \\mathbb R^{F^l} \\to \\mathbb R$  are learnable functions responsible for evaluating the reciprocal importance of two $r+1$-cells that share a common $(r)$-cell or are parts of the same $(r+2)$-cell.\n",
    "- $\\phi^t$ is a learnable function that updates the features of a cell.\n",
    "\n",
    "3) Attentional Pooling (performed after each message passing round of 2)):\n",
    "\n",
    "\\begin{align*}\n",
    "&游린\\textrm{ Message.} \\quad m_{x} &=& \\gamma^t(h_x^t) =\\\\\n",
    "                &&=& \\tau^t (a^t\\cdot h_x^t)\\\\\n",
    "&游릱\\textrm{ Update.} \\quad h_x^{t+1} &=&  m_{x}h_x^t, \\forall x\\in \\mathcal C_r^{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\gamma^t$ is a learnable function that computes the attention coefficients (self-scores) as defined in the original paper.\n",
    "- $\\tau^t$ is a non-linear function, $a$ are learnable parameters.\n",
    "- $C^{t+1}_r$ is the set of rank-$r$ cells of the coarse cell complex, defined keeping the rank-$r$ cells corresponding to the top-K self-scores $\\gamma^t(h_x^t)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Task:\n",
    "\n",
    "We train this model to perform entire complex classification on [`MUTAG` from the TUDataset](https://paperswithcode.com/dataset/mutag). This dataset contains:\n",
    "- 188 samples of chemical compounds represented as graphs,\n",
    "- with 7 discrete node features.\n",
    "\n",
    "The task is to predict the mutagenicity of each compound on Salmonella Typhimurium. We use a [\"GAT-like\" attention function](https://arxiv.org/abs/1710.10903) following the approach from [SAN](https://arxiv.org/abs/2203.07485). We implemented also a  [\"GATv2-like\" attention function](https://arxiv.org/abs/2105.14491).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:06:36.009880829Z",
     "start_time": "2023-05-31T09:06:34.285257706Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from toponetx.classes.cell_complex import CellComplex\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from models.can.can import CAN\n",
    "from models.utils.sparse import from_sparse\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:53.006542411Z",
     "start_time": "2023-05-31T09:13:52.963074076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "We import a subset of MUTAG, a benchmark dataset for graph classification. \n",
    "\n",
    "We then lift each graph into our topological domain of choice, here: a cell complex.\n",
    "\n",
    "We also retrieve:\n",
    "- input signals `x_0` and `x_1` on the nodes (0-cells) and edges (1-cells) for each complex: these will be the model's inputs,\n",
    "- a binary classification label `y` associated to the cell complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:55.279147916Z",
     "start_time": "2023-05-31T09:13:55.269057585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 36], x=[16, 7], edge_attr=[36, 4], y=[1])\n",
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
      "Features on nodes for the 0th cell complex: torch.Size([17, 7]).\n",
      "Features on edges for the 0th cell complex: torch.Size([38, 4]).\n",
      "Label of 0th cell complex: 1.\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(\n",
    "    root=\"/tmp/MUTAG\", name=\"MUTAG\", use_edge_attr=True, use_node_attr=True\n",
    ")\n",
    "dataset = dataset\n",
    "cc_list = []\n",
    "x_0_list = []\n",
    "x_1_list = []\n",
    "y_list = []\n",
    "for graph in dataset:\n",
    "    cell_complex = CellComplex(to_networkx(graph))\n",
    "    cc_list.append(cell_complex)\n",
    "    x_0_list.append(graph.x)\n",
    "    x_1_list.append(graph.edge_attr)\n",
    "    y_list.append(int(graph.y))\n",
    "else:\n",
    "    print(graph)\n",
    "\n",
    "i_cc = 0\n",
    "print(dataset[i_cc])\n",
    "print(f\"Features on nodes for the {i_cc}th cell complex: {x_0_list[i_cc].shape}.\")\n",
    "print(f\"Features on edges for the {i_cc}th cell complex: {x_1_list[i_cc].shape}.\")\n",
    "print(f\"Label of {i_cc}th cell complex: {y_list[i_cc]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutEdgeDataView([(0, 1, {}), (0, 5, {}), (1, 0, {}), (1, 2, {}), (2, 1, {}), (2, 3, {}), (2, 10, {}), (3, 2, {}), (3, 4, {}), (3, 7, {}), (4, 3, {}), (4, 5, {}), (5, 0, {}), (5, 4, {}), (5, 6, {}), (6, 5, {}), (7, 3, {}), (7, 8, {}), (7, 9, {}), (8, 7, {}), (9, 7, {}), (10, 2, {})])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_list[4].edges(data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features on nodes for the 4th cell complex: torch.Size([11, 7]).\n",
      "Features on edges for the 4th cell complex: torch.Size([22, 4]).\n",
      "Edge 0 between nodes 0 and 1: tensor([1., 0., 0., 0.])\n",
      "Edge 1 between nodes 0 and 5: tensor([1., 0., 0., 0.])\n",
      "Edge 2 between nodes 1 and 0: tensor([1., 0., 0., 0.])\n",
      "Edge 3 between nodes 1 and 2: tensor([1., 0., 0., 0.])\n",
      "Edge 4 between nodes 2 and 1: tensor([1., 0., 0., 0.])\n",
      "Edge 5 between nodes 2 and 3: tensor([1., 0., 0., 0.])\n",
      "Edge 6 between nodes 2 and 10: tensor([0., 1., 0., 0.])\n",
      "Edge 7 between nodes 3 and 2: tensor([1., 0., 0., 0.])\n",
      "Edge 8 between nodes 3 and 4: tensor([1., 0., 0., 0.])\n",
      "Edge 9 between nodes 3 and 7: tensor([0., 1., 0., 0.])\n",
      "Edge 10 between nodes 4 and 3: tensor([1., 0., 0., 0.])\n",
      "Edge 11 between nodes 4 and 5: tensor([1., 0., 0., 0.])\n",
      "Edge 12 between nodes 5 and 0: tensor([1., 0., 0., 0.])\n",
      "Edge 13 between nodes 5 and 4: tensor([1., 0., 0., 0.])\n",
      "Edge 14 between nodes 5 and 6: tensor([0., 1., 0., 0.])\n",
      "Edge 15 between nodes 6 and 5: tensor([0., 1., 0., 0.])\n",
      "Edge 16 between nodes 7 and 3: tensor([0., 1., 0., 0.])\n",
      "Edge 17 between nodes 7 and 8: tensor([0., 0., 1., 0.])\n",
      "Edge 18 between nodes 7 and 9: tensor([0., 1., 0., 0.])\n",
      "Edge 19 between nodes 8 and 7: tensor([0., 0., 1., 0.])\n",
      "Edge 20 between nodes 9 and 7: tensor([0., 1., 0., 0.])\n",
      "Edge 21 between nodes 10 and 2: tensor([0., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "i_cc = 4\n",
    "print(f\"Features on nodes for the {i_cc}th cell complex: {x_0_list[i_cc].shape}.\")\n",
    "print(f\"Features on edges for the {i_cc}th cell complex: {x_1_list[i_cc].shape}.\")\n",
    "# print(x_0_list[i_cc])\n",
    "# print(x_1_list[i_cc])\n",
    "# Assume dataset[i_cc] is your graph data object\n",
    "for i, edge in enumerate(dataset[i_cc].edge_index.t().tolist()):\n",
    "    print(f\"Edge {i} between nodes {edge[0]} and {edge[1]}: {dataset[i_cc].edge_attr[i]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Implementing CAN will require to perform message passing along neighborhood structures of the cell complexes.\n",
    "\n",
    "Thus, now we retrieve these neighborhood structures (i.e. their representative matrices) that we will use to send messages. \n",
    "\n",
    "We need the matrices $A_{\\downarrow, 1}$ and $A_{\\uparrow, 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:55.832585216Z",
     "start_time": "2023-05-31T09:13:55.815448708Z"
    }
   },
   "outputs": [],
   "source": [
    "lower_neighborhood_list = []\n",
    "upper_neighborhood_list = []\n",
    "adjacency_0_list = []\n",
    "\n",
    "for cell_complex in cc_list:\n",
    "    adjacency_0 = cell_complex.adjacency_matrix(rank=0)\n",
    "    adjacency_0 = torch.from_numpy(adjacency_0.todense()).to_sparse()\n",
    "    adjacency_0_list.append(adjacency_0)\n",
    "\n",
    "    lower_neighborhood_t = cell_complex.down_laplacian_matrix(rank=1)\n",
    "    lower_neighborhood_t = from_sparse(lower_neighborhood_t)\n",
    "    lower_neighborhood_list.append(lower_neighborhood_t)\n",
    "\n",
    "    try:\n",
    "        upper_neighborhood_t = cell_complex.up_laplacian_matrix(rank=1)\n",
    "        upper_neighborhood_t = from_sparse(upper_neighborhood_t)\n",
    "    except:\n",
    "        upper_neighborhood_t = np.zeros(\n",
    "            (lower_neighborhood_t.shape[0], lower_neighborhood_t.shape[0])\n",
    "        )\n",
    "        upper_neighborhood_t = torch.from_numpy(upper_neighborhood_t).to_sparse()\n",
    "\n",
    "    upper_neighborhood_list.append(upper_neighborhood_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutEdgeView([(0, 1), (0, 5), (1, 0), (1, 2), (2, 1), (2, 3), (2, 10), (3, 2), (3, 4), (3, 7), (4, 3), (4, 5), (5, 0), (5, 4), (5, 6), (6, 5), (7, 3), (7, 8), (7, 9), (8, 7), (9, 7), (10, 2)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_list[4].edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([], size=(2, 0)),\n",
      "       values=tensor([], size=(0,)),\n",
      "       size=(44, 44), nnz=0, dtype=torch.float64, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "print(upper_neighborhood_list[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the CANLayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:56.672913561Z",
     "start_time": "2023-05-31T09:13:56.667986426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of input features on nodes, edges and faces are: 7, 4 and 5.\n"
     ]
    }
   ],
   "source": [
    "in_channels_0 = x_0_list[0].shape[-1]\n",
    "in_channels_1 = x_1_list[0].shape[-1]\n",
    "in_channels_2 = 5\n",
    "print(\n",
    "    f\"The dimension of input features on nodes, edges and faces are: {in_channels_0}, {in_channels_1} and {in_channels_2}.\"\n",
    ")\n",
    "model = CAN(\n",
    "    in_channels_0,\n",
    "    in_channels_1,\n",
    "    32,\n",
    "    dropout=0.5,\n",
    "    heads=2,\n",
    "    num_classes=2,\n",
    "    n_layers=2,\n",
    "    att_lift=True,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, initialize loss, and specify an optimizer. We first try it without any attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:40.411845803Z",
     "start_time": "2023-05-31T09:19:40.408861921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAN(\n",
       "  (lift_layer): MultiHeadLiftLayer(\n",
       "    (lifts): LiftLayer()\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=11, out_features=64, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=11, out_features=64, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=11, out_features=64, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (1): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (2): PoolLayer(\n",
       "      (signal_pool_activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lin_0): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (lin_1): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:41.150933630Z",
     "start_time": "2023-05-31T09:19:41.146986990Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "x_1_train, x_1_test = train_test_split(x_1_list, test_size=test_size, shuffle=False)\n",
    "x_0_train, x_0_test = train_test_split(x_0_list, test_size=test_size, shuffle=False)\n",
    "lower_neighborhood_train, lower_neighborhood_test = train_test_split(\n",
    "    lower_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "upper_neighborhood_train, upper_neighborhood_test = train_test_split(\n",
    "    upper_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "adjacency_0_train, adjacency_0_test = train_test_split(\n",
    "    adjacency_0_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(y_list, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The number of epochs below have been kept low to facilitate debugging and testing. Real use cases should likely require more epochs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:42.918836083Z",
     "start_time": "2023-05-31T09:19:42.114801039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.6304 Train_acc: 0.6947\n",
      "Test_acc: 0.5965\n",
      "Epoch: 2 loss: 0.6058 Train_acc: 0.6947\n",
      "Test_acc: 0.5965\n",
      "Epoch: 3 loss: 0.6047 Train_acc: 0.6947\n",
      "Test_acc: 0.5965\n",
      "Epoch: 4 loss: 0.5954 Train_acc: 0.7099\n",
      "Test_acc: 0.6140\n",
      "Epoch: 5 loss: 0.5946 Train_acc: 0.7252\n",
      "Test_acc: 0.6140\n",
      "Epoch: 6 loss: 0.5880 Train_acc: 0.7252\n",
      "Test_acc: 0.6667\n",
      "Epoch: 7 loss: 0.5826 Train_acc: 0.7176\n",
      "Test_acc: 0.6491\n",
      "Epoch: 8 loss: 0.5788 Train_acc: 0.7328\n",
      "Test_acc: 0.6491\n",
      "Epoch: 9 loss: 0.5639 Train_acc: 0.7405\n",
      "Test_acc: 0.7895\n",
      "Epoch: 10 loss: 0.5628 Train_acc: 0.7405\n",
      "Test_acc: 0.7018\n",
      "Epoch: 11 loss: 0.5571 Train_acc: 0.7328\n",
      "Test_acc: 0.6316\n",
      "Epoch: 12 loss: 0.5539 Train_acc: 0.7252\n",
      "Test_acc: 0.6491\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\important codes\\Cell-Attention-Network-on-Molecule\\can_train.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/important%20codes/Cell-Attention-Network-on-Molecule/can_train.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m lower_neighborhood, upper_neighborhood \u001b[39m=\u001b[39m lower_neighborhood\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/important%20codes/Cell-Attention-Network-on-Molecule/can_train.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     device\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/important%20codes/Cell-Attention-Network-on-Molecule/can_train.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ), upper_neighborhood\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/important%20codes/Cell-Attention-Network-on-Molecule/can_train.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/important%20codes/Cell-Attention-Network-on-Molecule/can_train.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model(x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/important%20codes/Cell-Attention-Network-on-Molecule/can_train.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m crit(y_hat, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/important%20codes/Cell-Attention-Network-on-Molecule/can_train.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (y_hat\u001b[39m.\u001b[39margmax() \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\33433\\anaconda3\\envs\\tmx-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\important codes\\Cell-Attention-Network-on-Molecule\\models\\can\\can.py:140\u001b[0m, in \u001b[0;36mCAN.forward\u001b[1;34m(self, x_0, x_1, neighborhood_0_to_0, lower_neighborhood, upper_neighborhood)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, PoolLayer):\n\u001b[1;32m--> 140\u001b[0m         x_1, lower_neighborhood, upper_neighborhood \u001b[39m=\u001b[39m layer(\n\u001b[0;32m    141\u001b[0m             x_1, lower_neighborhood, upper_neighborhood\n\u001b[0;32m    142\u001b[0m         )\n\u001b[0;32m    143\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m         x_1 \u001b[39m=\u001b[39m layer(x_1, lower_neighborhood, upper_neighborhood)\n",
      "File \u001b[1;32mc:\\Users\\33433\\anaconda3\\envs\\tmx-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\important codes\\Cell-Attention-Network-on-Molecule\\models\\can\\can_layer.py:382\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x_0, lower_neighborhood, upper_neighborhood)\u001b[0m\n\u001b[0;32m    374\u001b[0m lower_neighborhood_modified \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(\n\u001b[0;32m    375\u001b[0m     lower_neighborhood_modified, \u001b[39m1\u001b[39m, top_indices\n\u001b[0;32m    376\u001b[0m )\n\u001b[0;32m    377\u001b[0m upper_neighborhood_modified \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(\n\u001b[0;32m    378\u001b[0m     upper_neighborhood, \u001b[39m0\u001b[39m, top_indices\n\u001b[0;32m    379\u001b[0m )\n\u001b[0;32m    380\u001b[0m upper_neighborhood_modified \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(\n\u001b[0;32m    381\u001b[0m     upper_neighborhood_modified, \u001b[39m1\u001b[39m, top_indices\n\u001b[1;32m--> 382\u001b[0m )\n\u001b[0;32m    383\u001b[0m \u001b[39m# return sparse matrices of neighborhood\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    385\u001b[0m     out,\n\u001b[0;32m    386\u001b[0m     lower_neighborhood_modified\u001b[39m.\u001b[39mto_sparse()\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcoalesce(),\n\u001b[0;32m    387\u001b[0m     upper_neighborhood_modified\u001b[39m.\u001b[39mto_sparse()\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcoalesce(),\n\u001b[0;32m    388\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 50\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        adjacency_0_train,\n",
    "        lower_neighborhood_train,\n",
    "        upper_neighborhood_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0 = x_0.float().to(device)\n",
    "        x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
    "        adjacency = adjacency.float().to(device)\n",
    "        lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "            device\n",
    "        ), upper_neighborhood.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood)\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += (y_hat.argmax() == y).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                adjacency_0_test,\n",
    "                lower_neighborhood_test,\n",
    "                upper_neighborhood_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0 = x_0.float().to(device)\n",
    "                x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(\n",
    "                    device\n",
    "                )\n",
    "                adjacency = adjacency.float().to(device)\n",
    "                lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "                    device\n",
    "                ), upper_neighborhood.float().to(device)\n",
    "                y_hat = model(\n",
    "                    x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood\n",
    "                )\n",
    "                \n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/qm9_test_cell_complex.pkl\", \"rb\") as f:\n",
    "    cc_list = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "97e7f600578393f7b22fad5e1bb04e54aa849deabd28651fd7e27af1b0c8a034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
