{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.can.can import CAN\n",
    "from models.utils.sparse import from_sparse\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/qm9_train_cell_complex.pkl\", \"rb\") as f:\n",
    "    cc_list = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_loader import CCDataset\n",
    "cc_list = cc_list[:100]\n",
    "dataset = CCDataset(cc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0_list = [data[0] for data in dataset]\n",
    "x_1_list = [data[1] for data in dataset]\n",
    "y_list = [data[5] for data in dataset]\n",
    "\n",
    "lower_neighborhood_list = []\n",
    "upper_neighborhood_list = []\n",
    "adjacency_0_list = []\n",
    "\n",
    "for cell_complex in cc_list:\n",
    "    adjacency_0 = cell_complex.adjacency_matrix(rank=0)\n",
    "    adjacency_0 = torch.from_numpy(adjacency_0.todense()).to_sparse()\n",
    "    adjacency_0_list.append(adjacency_0)\n",
    "\n",
    "    lower_neighborhood_t = cell_complex.down_laplacian_matrix(rank=1, signed=False)\n",
    "    lower_neighborhood_t = from_sparse(lower_neighborhood_t)\n",
    "    lower_neighborhood_list.append(lower_neighborhood_t)\n",
    "\n",
    "    try:\n",
    "        upper_neighborhood_t = cell_complex.up_laplacian_matrix(rank=1, signed=False)\n",
    "        upper_neighborhood_t = from_sparse(upper_neighborhood_t)\n",
    "    except:\n",
    "        upper_neighborhood_t = np.zeros(\n",
    "            (lower_neighborhood_t.shape[0], lower_neighborhood_t.shape[0])\n",
    "        )\n",
    "        upper_neighborhood_t = torch.from_numpy(upper_neighborhood_t).to_sparse()\n",
    "\n",
    "    upper_neighborhood_list.append(upper_neighborhood_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_0 = x_0_list[0].shape[-1]\n",
    "in_channels_1 = x_1_list[0].shape[-1]\n",
    "#in_channels_2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels_0, in_channels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAN(\n",
    "    in_channels_0,\n",
    "    in_channels_1,\n",
    "    32,\n",
    "    dropout=0.5,\n",
    "    heads=5,\n",
    "    n_layers=3,\n",
    "    att_lift=True,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAN(\n",
       "  (lift_layer): MultiHeadLiftLayer(\n",
       "    (lifts): LiftLayer()\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention_v2(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin_src): Linear(in_features=7, out_features=160, bias=False)\n",
       "        (lin_dst): Linear(in_features=7, out_features=160, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention_v2(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin_src): Linear(in_features=7, out_features=160, bias=False)\n",
       "        (lin_dst): Linear(in_features=7, out_features=160, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=7, out_features=160, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (1): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention_v2(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin_src): Linear(in_features=160, out_features=160, bias=False)\n",
       "        (lin_dst): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention_v2(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin_src): Linear(in_features=160, out_features=160, bias=False)\n",
       "        (lin_dst): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=160, out_features=160, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (2): PoolLayer(\n",
       "      (signal_pool_activation): Sigmoid()\n",
       "    )\n",
       "    (3): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention_v2(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin_src): Linear(in_features=160, out_features=160, bias=False)\n",
       "        (lin_dst): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention_v2(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin_src): Linear(in_features=160, out_features=160, bias=False)\n",
       "        (lin_dst): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=160, out_features=160, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (4): PoolLayer(\n",
       "      (signal_pool_activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lin_0): Linear(in_features=160, out_features=128, bias=True)\n",
       "  (lin_1): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit = torch.nn.L1Loss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283649\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "x_1_train, x_1_test = train_test_split(x_1_list, test_size=test_size, shuffle=False)\n",
    "x_0_train, x_0_test = train_test_split(x_0_list, test_size=test_size, shuffle=False)\n",
    "lower_neighborhood_train, lower_neighborhood_test = train_test_split(\n",
    "    lower_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "upper_neighborhood_train, upper_neighborhood_test = train_test_split(\n",
    "    upper_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "adjacency_0_train, adjacency_0_test = train_test_split(\n",
    "    adjacency_0_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(y_list, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33433\\anaconda3\\envs\\tmx-test\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.0905\n",
      "Test loss: 0.2452\n",
      "Epoch: 2 loss: 0.0685\n",
      "Test loss: 0.2705\n",
      "Epoch: 3 loss: 0.0653\n",
      "Test loss: 0.2798\n",
      "Epoch: 4 loss: 0.0588\n",
      "Test loss: 0.2837\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 4\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        adjacency_0_train,\n",
    "        lower_neighborhood_train,\n",
    "        upper_neighborhood_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        i = i + 1\n",
    "        #print(i)\n",
    "        x_0 = x_0.float().to(device)\n",
    "        x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.float).to(device)\n",
    "        adjacency = adjacency.float().to(device)\n",
    "        lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "            device\n",
    "        ), upper_neighborhood.float().to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood)\n",
    "        #print('notice')\n",
    "        loss = crit(y_hat, y)\n",
    "        #print(loss)\n",
    "\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            #num_samples = 0\n",
    "            test_loss = []\n",
    "            for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                adjacency_0_test,\n",
    "                lower_neighborhood_test,\n",
    "                upper_neighborhood_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0 = x_0.float().to(device)\n",
    "                x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(\n",
    "                    device\n",
    "                )\n",
    "                adjacency = adjacency.float().to(device)\n",
    "                lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "                    device\n",
    "                ), upper_neighborhood.float().to(device)\n",
    "                y_hat = model(\n",
    "                    x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood\n",
    "                )\n",
    "                #print(y_hat)\n",
    "                loss = crit(y_hat, y)\n",
    "                test_loss.append(loss.item())\n",
    "                #num_samples += 1\n",
    "            print(f\"Test loss: {np.mean(test_loss):.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.can.can_layer import MultiHeadCellAttention_v2\n",
    "\n",
    "mh = MultiHeadCellAttention_v2(in_channels=3, out_channels = 32, \n",
    "                               heads = 3, concat= True,\n",
    "                               att_activation= torch.nn.ReLU(), aggr_func='sum',\n",
    "                               dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([], size=(2, 0)),\n",
      "       values=tensor([], size=(0,)),\n",
      "       size=(2, 2), nnz=0, dtype=torch.float64, layout=torch.sparse_coo)\n",
      "tensor(indices=tensor([[0, 0, 1, 1],\n",
      "                       [0, 1, 0, 1]]),\n",
      "       values=tensor([2., 2., 2., 2.]),\n",
      "       size=(2, 2), nnz=4, layout=torch.sparse_coo)\n",
      "CellView([])\n",
      "2\n",
      "torch.Size([2, 96])\n",
      "torch.Size([2, 96])\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(upper_neighborhood_list[i])\n",
    "print(lower_neighborhood_list[i])\n",
    "print(cc_list[i].cells)\n",
    "print(cc_list[i].number_of_edges())\n",
    "print(mh(x_1_list[i], upper_neighborhood_list[i]).shape)\n",
    "print(mh(x_1_list[i], lower_neighborhood_list[i]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_edges: 18\n"
     ]
    }
   ],
   "source": [
    "class CustomTensor:\n",
    "    def __init__(self, tensor, n_edges=None):\n",
    "        self.tensor = tensor\n",
    "        self.n_edges = n_edges\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # Delegate attribute access to the tensor\n",
    "        return getattr(self.tensor, name)\n",
    "\n",
    "    # If you need to support specific tensor methods, you might need to explicitly define them\n",
    "    def to(self, *args, **kwargs):\n",
    "        # Example of delegating a method call to the tensor\n",
    "        self.tensor = self.tensor.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "# Usage\n",
    "your_tensor = torch.tensor([1, 2, 3])\n",
    "n_edges = 18\n",
    "custom_tensor = CustomTensor(your_tensor, n_edges=n_edges)\n",
    "\n",
    "# Tensor operations\n",
    "result = custom_tensor.tensor + torch.tensor([1, 1, 1])\n",
    "\n",
    "# Accessing custom attribute\n",
    "print(\"n_edges:\", custom_tensor.n_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def scatter_add_(self, index, src, num_edges=None):\n",
    "    if num_edges is None:\n",
    "        num_edges = src.numel()\n",
    "\n",
    "    # Check if dimensions match and num_edges is within the valid range\n",
    "    if self.dim() != src.dim() or self.dim() != index.dim():\n",
    "        raise ValueError(\"All tensors must have the same number of dimensions\")\n",
    "    if num_edges > src.numel():\n",
    "        raise ValueError(\"num_edges is larger than the number of elements in src\")\n",
    "\n",
    "    # Iterate and add values\n",
    "    for n in range(num_edges):\n",
    "        # Convert flat index to 3D index\n",
    "        i, j, k = np.unravel_index(n, src.shape)\n",
    "        self[index[i][j][k]][j][k] += src[i][j][k]\n",
    "\n",
    "    return self\n",
    "\n",
    "# Example usage\n",
    "self_tensor = torch.zeros(5, 3, 3)  # Adjust the size as needed\n",
    "index_tensor = torch.randint(0, 5, (2, 3, 3))\n",
    "src_tensor = torch.randn(2, 3, 3)\n",
    "\n",
    "scatter_add_(self_tensor, index_tensor, src_tensor).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
