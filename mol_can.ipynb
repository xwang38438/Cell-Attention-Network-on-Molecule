{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.can.can import CAN\n",
    "from models.utils.sparse import from_sparse\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/qm9_test_cell_complex.pkl\", \"rb\") as f:\n",
    "    cc_list = pickle.load(f)\n",
    "\n",
    "# take only first 30\n",
    "cc_list = cc_list[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ndsjsj import CCDataset\n",
    "dataset = CCDataset(cc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0_list = [data[0] for data in dataset]\n",
    "x_1_list = [data[1] for data in dataset]\n",
    "y_list = [data[5] for data in dataset]\n",
    "\n",
    "lower_neighborhood_list = []\n",
    "upper_neighborhood_list = []\n",
    "adjacency_0_list = []\n",
    "\n",
    "for cell_complex in cc_list:\n",
    "    adjacency_0 = cell_complex.adjacency_matrix(rank=0)\n",
    "    adjacency_0 = torch.from_numpy(adjacency_0.todense()).to_sparse()\n",
    "    adjacency_0_list.append(adjacency_0)\n",
    "\n",
    "    lower_neighborhood_t = cell_complex.down_laplacian_matrix(rank=1)\n",
    "    lower_neighborhood_t = from_sparse(lower_neighborhood_t)\n",
    "    lower_neighborhood_list.append(lower_neighborhood_t)\n",
    "\n",
    "    try:\n",
    "        upper_neighborhood_t = cell_complex.up_laplacian_matrix(rank=1)\n",
    "        upper_neighborhood_t = from_sparse(upper_neighborhood_t)\n",
    "    except:\n",
    "        upper_neighborhood_t = np.zeros(\n",
    "            (lower_neighborhood_t.shape[0], lower_neighborhood_t.shape[0])\n",
    "        )\n",
    "        upper_neighborhood_t = torch.from_numpy(upper_neighborhood_t).to_sparse()\n",
    "\n",
    "    upper_neighborhood_list.append(upper_neighborhood_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_0 = x_0_list[0].shape[-1]\n",
    "in_channels_1 = x_1_list[0].shape[-1]\n",
    "#in_channels_2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels_0, in_channels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAN(\n",
    "    in_channels_0,\n",
    "    in_channels_1,\n",
    "    32,\n",
    "    dropout=0.5,\n",
    "    heads=3,\n",
    "    num_classes=1,\n",
    "    n_layers=2,\n",
    "    att_lift=False,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAN(\n",
       "  (layers): ModuleList(\n",
       "    (0): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=3, out_features=96, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=3, out_features=96, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=3, out_features=96, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (1): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=96, out_features=96, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (2): PoolLayer(\n",
       "      (signal_pool_activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lin_0): Linear(in_features=96, out_features=128, bias=True)\n",
       "  (lin_1): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define mae loss\n",
    "crit = torch.nn.L1Loss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "x_1_train, x_1_test = train_test_split(x_1_list, test_size=test_size, shuffle=False)\n",
    "x_0_train, x_0_test = train_test_split(x_0_list, test_size=test_size, shuffle=False)\n",
    "lower_neighborhood_train, lower_neighborhood_test = train_test_split(\n",
    "    lower_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "upper_neighborhood_train, upper_neighborhood_test = train_test_split(\n",
    "    upper_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "adjacency_0_train, adjacency_0_test = train_test_split(\n",
    "    adjacency_0_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(y_list, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allenwang/miniconda3/envs/tmx/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [9, 96] at entry 0 and [8, 96] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m lower_neighborhood, upper_neighborhood \u001b[38;5;241m=\u001b[39m lower_neighborhood\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     20\u001b[0m     device\n\u001b[1;32m     21\u001b[0m ), upper_neighborhood\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_neighborhood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_neighborhood\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(y_hat, y)\n\u001b[1;32m     25\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y_hat\u001b[38;5;241m.\u001b[39margmax() \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/tmx/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/Cell-Attention-Network-on-Molecule/models/can/can.py:144\u001b[0m, in \u001b[0;36mCAN.forward\u001b[0;34m(self, x_0, x_1, neighborhood_0_to_0, lower_neighborhood, upper_neighborhood)\u001b[0m\n\u001b[1;32m    140\u001b[0m         x_1, lower_neighborhood, upper_neighborhood \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m    141\u001b[0m             x_1, lower_neighborhood, upper_neighborhood\n\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         x_1 \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_neighborhood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_neighborhood\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         x_1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x_1, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# max pooling over all nodes in each graph\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tmx/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/Cell-Attention-Network-on-Molecule/models/can/can_layer.py:965\u001b[0m, in \u001b[0;36mCANLayer.forward\u001b[0;34m(self, x, lower_neighborhood, upper_neighborhood)\u001b[0m\n\u001b[1;32m    961\u001b[0m     w_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m    963\u001b[0m \u001b[38;5;66;03m# between-neighborhood aggregation and update\u001b[39;00m\n\u001b[1;32m    964\u001b[0m out \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlower_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_x\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregation([lower_x, upper_x])\n\u001b[1;32m    968\u001b[0m )\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/tmx/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/Cell-Attention-Network-on-Molecule/models/base/aggregation.py:62\u001b[0m, in \u001b[0;36mAggregation.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Aggregated messages.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     64\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(x), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [9, 96] at entry 0 and [8, 96] at entry 1"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 2\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        adjacency_0_train,\n",
    "        lower_neighborhood_train,\n",
    "        upper_neighborhood_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0 = x_0.float().to(device)\n",
    "        x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
    "        adjacency = adjacency.float().to(device)\n",
    "        lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "            device\n",
    "        ), upper_neighborhood.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood)\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += (y_hat.argmax() == y).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                adjacency_0_test,\n",
    "                lower_neighborhood_test,\n",
    "                upper_neighborhood_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0 = x_0.float().to(device)\n",
    "                x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(\n",
    "                    device\n",
    "                )\n",
    "                adjacency = adjacency.float().to(device)\n",
    "                lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "                    device\n",
    "                ), upper_neighborhood.float().to(device)\n",
    "                y_hat = model(\n",
    "                    x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood\n",
    "                )\n",
    "                print(y_hat)\n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1_train[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
